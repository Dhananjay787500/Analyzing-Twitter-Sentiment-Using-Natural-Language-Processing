{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlKstzQfULew",
        "outputId": "bb586ec3-271b-40b4-c127-4241e38d53c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importing the Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUIIfeVWURET",
        "outputId": "e3622eda-6f55-429a-a379-403a4fd49217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27481, 4)\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Tweets.csv', encoding='latin-1')\n",
        "\n",
        "# Rows and column\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJuAYC0aW1aD"
      },
      "outputs": [],
      "source": [
        "# Initialize Lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdC7NwskUXm_",
        "outputId": "3c644d69-c1f3-4ca0-90ee-68e6ca46c012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text 1:  I`d have responded, if I were going\n",
            "Cleaned Text 1: id responded going\n",
            "\n",
            "Original Text 2:  Sooo SAD I will miss you here in San Diego!!!\n",
            "Cleaned Text 2: sooo sad miss san diego\n",
            "\n",
            "Original Text 3: my boss is bullying me...\n",
            "Cleaned Text 3: bos bullying\n",
            "\n",
            "Original Text 4:  what interview! leave me alone\n",
            "Cleaned Text 4: interview leave alone\n",
            "\n",
            "Original Text 5:  Sons of ****, why couldn`t they put them on the releases we already bought\n",
            "Cleaned Text 5: son couldnt put release already bought\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Check if text is a string before applying preprocessing\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()  # Lowercase\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "        tokens = word_tokenize(text)  # Tokenize\n",
        "        tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        # Return empty string or handle NaN values as needed\n",
        "        return ''\n",
        "# Apply preprocessing\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Display cleaned data (first five observations)\n",
        "for i in range(5):\n",
        "    original_text = df.loc[i, 'text']\n",
        "    cleaned_text = preprocess_text(original_text)\n",
        "    print(f\"Original Text {i+1}: {original_text}\")\n",
        "    print(f\"Cleaned Text {i+1}: {cleaned_text}\\n\")\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('Clean_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN0apyXgelQx",
        "outputId": "89ab4d80-343f-4e89-f5dc-4238302941c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7014\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.62      0.67      1589\n",
            "           1       0.64      0.72      0.68      2150\n",
            "           2       0.77      0.75      0.76      1744\n",
            "\n",
            "    accuracy                           0.70      5483\n",
            "   macro avg       0.71      0.70      0.70      5483\n",
            "weighted avg       0.71      0.70      0.70      5483\n",
            "\n",
            "Model and vectorizer saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv(\"Clean_data.csv\")\n",
        "\n",
        "# Drop missing values in important columns\n",
        "df = df.dropna(subset=['cleaned_text', 'sentiment'])\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(df['cleaned_text'])\n",
        "\n",
        "# Encode sentiment labels\n",
        "y = df['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model with optimized parameters\n",
        "model = RandomForestClassifier(n_estimators=50, random_state=42)  # Reduced estimators for speed\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, \"sentiment_model.pkl\")\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"Model and vectorizer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FafSBiOYhSjB",
        "outputId": "fe4c08c6-9be3-45db-da47-8a1f36c4e5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h35.225.209.177\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q\n",
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5YGCfWPhZ37",
        "outputId": "67475de5-cdcb-4683-fe70-c0d8caed3311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.225.209.177:8501\u001b[0m\n",
            "\u001b[0m\n",
            "y\n",
            "\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\n",
            "\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0Kyour url is: https://hungry-ducks-bet.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run UI.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}